# Agentic SQL Chatbot Workflow

This directory contains the three JSON workflow files required to run the **Text-to-SQL AI Agent**.

## Workflow Architecture

Instead of a single massive workflow, this agent is modularized into a **Main T2SQL Agent** and **Two Tool Wrappers**.

### Why Separate Workflows?

We decoupled the tools into sub-workflows ("Wrappers") for two critical engineering reasons:

1.  **Input Definition for 8B Models:** The native n8n Postgres nodes are strict. Smaller models like Llama 3.1 8B often hallucinate inputs (e.g., sending arguments to a tool that requires none). By wrapping the tool, we could define custom inputs (like `unused_input`) to "catch" these hallucinations without crashing the agent.
2.  **Logic Injection:** A simple node cannot handle complex logic like sanitizing inputs, checking for `DELETE` commands, or handling empty database results. Wrapping them allows us to build a "Logic Layer" around every database interaction.


## The Workflows

### 1\. `Main_T2SQL_Agent.json` (The Brain)
<img width="1503" height="680" alt="image" src="https://github.com/user-attachments/assets/c43497af-6ec4-4bc8-b258-cc8cae91acf2" />

This workflow hosts the **AI Agent** node. It handles the chat memory, user input, and reasoning loop.
  * **Model:** Llama 3.1 (via Groq) or any other model
  * **Prompting Strategy:** Uses a manual **Chain of Thought (CoT)** system prompt. It forces the AI to:
    1.  Inspect Schema first.
    2.  Formulate a plan.
    3.  Execute the query.
    4.  Summarize results.
  * **Search Protocol:** The system prompt explicitly enforces `ILIKE` logic (`WHERE name ILIKE '%query%'`) to handle case-insensitive searches (e.g., finding "Gaming Laptop" when the user asks for "gaming laptop").
  * **Auto-Correction & Retry Logic:** We designed the system to be resilient against the "lazy" nature of smaller LLMs (like Llama 3 8B).
     * **The Problem:** Smaller models often generate invalid SQL (e.g., using Markdown backticks or correct syntax for MySQL but not Postgres).
     * **The Solution:** When the *Run SQL Safe* tool returns a JSON error (e.g., `{"error": "syntax error..."}`), the Main Agent is instructed via the System Prompt to treat this as feedback, not a failure.
     * **Configuration:** The Agent node is configured with `Max Iterations: 10`. This gives the AI five "turns" to try, fail, read the error, fix the query, and try again, creating a self-healing loop.
  * **Contextual Awareness (Simple Memory):** The Main Agent utilizes a **Simple Memory** node (Window Buffer) connected to the agent.
     * **Purpose:** It retains the last `N` turns of conversation.
     * **Why it matters:** This allows the user to ask follow-up questions like "Who bought it?" or "How much does that cost?" without re-stating the context. The AI remembers the previous SQL query results and understands what "it" or "that" refers to.

### 2\. `Tool_Get_Schema.json`

<img width="600" height="200" alt="image" src="https://github.com/user-attachments/assets/4f22636d-0593-43c2-95f9-49c1aedafc8b" />

**Purpose:** Fetches table names and columns to ground the AI's knowledge.
**Key Features:**

  * **The "Dummy Input" Fix:** Llama 3.1 8B often tries to send input data to this tool even though none is required. We added a dummy field (`unused_input`) in the trigger node so n8n accepts the request instead of throwing a "Schema Mismatch" error.

### 3\. `Tool_Run_SQL_Safe.json`

<img width="2387" height="693" alt="image" src="https://github.com/user-attachments/assets/e9150f88-486a-4a3e-a1c8-7f5cf8a676ea" />

**Purpose:** Executes the SQL query generated by the AI. This workflow contains the bulk of the safety logic.

**Implementation Details:**

  * **Sanitization Layer:** An `Edit Fields` node strips Markdown backticks (\`\`\`sql) and whitespace from the AI's input using JavaScript `.replace()`.
  * **Security Guardrail:** An `If` node verifies the query starts with `select` (case-insensitive).
      * *Implementation:* We used `{{ $json.query.trim().toLowerCase() }}` -\> **Starts With** -\> `select`. This effectively blocks `DELETE`, `DROP`, or `INSERT` commands.
      
  * **The Syntax Check Layer:** Before executing any query against the live data, this workflow implements a "Dry Run" safety check using the PostgreSQL `EXPLAIN` command. This technique detects syntax errors without risking side effects.
    *   **The "Dry Run" Node:** We run `EXPLAIN {{ query }}` in a Postgres node. *Why?* If the SQL is invalid (e.g., typos like `SELETC` or referencing non-existent columns), Postgres returns an error immediately. Since `EXPLAIN` only plans the query, it is fast and safe. "On Error" is set to `Continue`. This prevents the workflow from crashing and allows us to capture the specific error message.
    *   **The Syntax Logic Gate (If Node):** We check if the previous `EXPLAIN` node output an error object (`$json.error` is not empty). **If Error:** The workflow routes to a "Return SQL Error" node, which formats the exact Postgres error message (e.g., `syntax error at or near "LIMIT"`) and sends it back to the AI Agent.
  * **Empty Result Handling:**
      * **The Problem:** Postgres returns nothing (0 items) if no rows match.
      * **The Fix:** We enabled "Always Output Data" on the Postgres node. An `If` node checks `{{ Object.keys($json).length == 0 }}`. If true, it returns a hardcoded `{ "result": "No Output" }` JSON to the AI, preventing it from hallucinating fake data.
  * **Error Reporting:** If the query fails (syntax error), the workflow catches the error (using "On Error: Continue") and passes the exact error message back to the AI so it can self-correct.


## Logic & Data Flow

The core agent uses a **"ReAct"** (Reasoning + Acting) approach.

```mermaid
flowchart TD
    %% Style Definitions
    classDef default fill:#fff,stroke:#333,stroke-width:1px,color:#000
    classDef agent fill:#ffdfba,stroke:#ffb347,stroke-width:2px,color:#000
    classDef db fill:#bae1ff,stroke:#4b90e2,stroke-width:2px,color:#000
    classDef error fill:#ffb3ba,stroke:#ff6961,stroke-width:2px,color:#000
    classDef success fill:#baffc9,stroke:#77dd77,stroke-width:2px,color:#000
    classDef tool fill:#e0e0e0,stroke:#9e9e9e,stroke-width:1px,color:#000

    %% Main Flow
    User[User] -->|Chat Message| Agent[AI Agent]
    
    %% Tool 1: Schema
    Agent -->|1. Inspect| SchemaTool[Tool: Get Schema]
    SchemaTool -->|Return Tables| Agent
    
    %% Tool 2: SQL Safe Logic
    Agent -->|2. Reason| Normalize
    
    subgraph SQLLogic [Tool: Run SQL Safe]
        direction TB
        Normalize[Normalize Input] --> Sanitize[Sanitize SQL]
        Sanitize --> CheckReadOnly{"Starts with<br/>SELECT?"}
        
        %% Branching
        CheckReadOnly -->|No| SecErr[Return Security Error]
        CheckReadOnly -->|Yes| Explain["Postgres: Explain"]
        
        Explain --> SyntaxCheck{"Syntax OK?"}
        SyntaxCheck -->|No| SQLErr[Return SQL Error]
        SyntaxCheck -->|Yes| Exec[Postgres: Execute]
        
        Exec --> EmptyCheck{"Result > 0?"}
        EmptyCheck -->|No| NoRecs[Return 'No Output' as JSON]
        EmptyCheck -->|Yes| Data[Return JSON Data]
    end

    %% Returns to Agent
    SecErr --> Agent
    SQLErr --> Agent
    NoRecs --> Agent
    Data --> Agent
    
    Agent -->|Answer| User

    %% Apply Styles (The Safe Way)
    class Agent agent
    class SchemaTool tool
    class SecErr,SQLErr error
    class Explain,Exec db
    class NoRecs,Data success
```

## Setup & Import Instructions

Because these workflows rely on calling each other, the links will break upon import (as n8n assigns new IDs to every node).

**Follow this sequence:**

1.  **Import Files:** Import all three JSON files into your n8n editor.
2.  **Re-Link Tools:**
      * Open the `Main_Agent` workflow.
      * Double-click the **Call n8n Workflow Tool** node named `get_db_schema`.
          * In the "Workflow" dropdown, select the `Tool_Get_Schema` workflow you just imported.
      * Double-click the **Call n8n Workflow Tool** node named `run_sql_query`.
          * In the "Workflow" dropdown, select the `Tool_Run_SQL_Safe` workflow.
3.  **Verify Credentials:** Ensure the Postgres nodes inside the two Tool workflows are connected to your Postgres Credential (`Host: postgres`, `User: n8n`, `Database: n8n`).
4.  **Save** all workflows.
5.  **Activate:** Toggle the `Active` switch on the Main Agent to start listening for chat messages.







